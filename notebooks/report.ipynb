{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m3es0as6uv",
   "source": "## Summary\n\nThis report analyzed the performance of DSPy taste distillation models across different configurations:\n\n### Key Metrics\n- **Baseline Performance**: Models using only the taste rubric without training\n- **Optimized Performance**: Models trained using DSPy optimizers (BootstrapFinetune, etc.)\n- **Tasks**: Favorite prediction and shortlist prediction\n- **Model Types**: Binary classification and pairwise comparison\n\n### Methodology\n1. Parsed evaluation logs from `saved/logs/` directory\n2. Extracted performance metrics, model types, and evaluation parameters\n3. Compared baseline vs optimized model performance\n4. Analyzed performance by task type and optimizer used\n5. Performed statistical significance testing\n\n### Next Steps\n1. **Model Selection**: Use the best performing models for production inference\n2. **Hyperparameter Tuning**: Experiment with different optimizer parameters\n3. **Data Augmentation**: Collect more training examples for better performance\n4. **Error Analysis**: Analyze failure cases to improve model robustness\n5. **Ensemble Methods**: Combine multiple models for better predictions\n\nThe analysis provides a comprehensive view of how DSPy optimization affects taste distillation performance, helping guide future model development and deployment decisions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xfhoah21wps",
   "source": "# Statistical Analysis and Key Insights\nprint(\"STATISTICAL ANALYSIS\")\nprint(\"=\" * 50)\n\n# Calculate performance improvements\nif len(baseline_results) > 0 and len(optimized_results) > 0:\n    baseline_mean = baseline_results['accuracy'].mean()\n    optimized_mean = optimized_results['accuracy'].mean()\n    improvement = optimized_mean - baseline_mean\n    improvement_pct = (improvement / baseline_mean) * 100\n    \n    print(f\"Overall Performance Improvement:\")\n    print(f\"  Baseline Mean Accuracy: {baseline_mean:.3f}\")\n    print(f\"  Optimized Mean Accuracy: {optimized_mean:.3f}\")\n    print(f\"  Absolute Improvement: {improvement:.3f}\")\n    print(f\"  Relative Improvement: {improvement_pct:.1f}%\")\n    \n    # Statistical significance test\n    from scipy import stats\n    t_stat, p_value = stats.ttest_ind(optimized_results['accuracy'], baseline_results['accuracy'])\n    print(f\"  t-statistic: {t_stat:.3f}\")\n    print(f\"  p-value: {p_value:.3f}\")\n    print(f\"  Statistically significant: {'Yes' if p_value < 0.05 else 'No'}\")\n\n# Best performing models\nprint(\"\\\\n\" + \"=\" * 50)\nprint(\"BEST PERFORMING MODELS\")\nprint(\"=\" * 50)\n\nif len(optimized_results) > 0:\n    best_models = optimized_results.nlargest(5, 'accuracy')\n    print(\"Top 5 Model Performances:\")\n    for idx, row in best_models.iterrows():\n        print(f\"  {row['model_name']}: {row['accuracy']:.3f} accuracy ({row['model_type']})\")\n\n# Performance by optimizer type\nif len(model_metadata) > 0 and 'optimizer_type' in model_metadata.columns:\n    print(\"\\\\n\" + \"=\" * 50)\n    print(\"PERFORMANCE BY OPTIMIZER TYPE\")\n    print(\"=\" * 50)\n    \n    if 'model_name' in optimized_results.columns:\n        perf_with_optimizer = optimized_results.merge(\n            model_metadata[['model_name', 'optimizer_type']], \n            on='model_name', \n            how='left'\n        )\n        \n        optimizer_stats = perf_with_optimizer.groupby('optimizer_type')['accuracy'].agg(['count', 'mean', 'std'])\n        for optimizer, stats in optimizer_stats.iterrows():\n            print(f\"  {optimizer}: {stats['mean']:.3f} Â± {stats['std']:.3f} (n={stats['count']})\")\n\nprint(\"\\\\n\" + \"=\" * 50)\nprint(\"KEY FINDINGS\")\nprint(\"=\" * 50)\n\nfindings = []\nif len(baseline_results) > 0 and len(optimized_results) > 0:\n    if improvement > 0:\n        findings.append(f\"DSPy optimization provides {improvement_pct:.1f}% improvement over baseline\")\n    else:\n        findings.append(\"DSPy optimization shows no significant improvement over baseline\")\n\nif len(optimized_results) > 0:\n    best_accuracy = optimized_results['accuracy'].max()\n    findings.append(f\"Best performing model achieved {best_accuracy:.3f} accuracy\")\n\n# Task-specific findings\nfor task in eval_results['task'].unique():\n    if task != 'Unknown':\n        task_results = eval_results[eval_results['task'] == task]\n        if len(task_results) > 0:\n            task_best = task_results['accuracy'].max()\n            findings.append(f\"Best {task} performance: {task_best:.3f}\")\n\nfor i, finding in enumerate(findings, 1):\n    print(f\"{i}. {finding}\")\n\nprint(\"\\\\n\" + \"=\" * 50)\nprint(\"RECOMMENDATIONS\")\nprint(\"=\" * 50)\n\nrecommendations = [\n    \"Focus on models with accuracy > 0.60 for production use\",\n    \"Consider ensemble methods combining multiple optimized models\",\n    \"Investigate the impact of training data size on performance\",\n    \"Analyze failure cases to improve rubric quality\",\n    \"Experiment with different DSPy optimizers for better results\"\n]\n\nfor i, rec in enumerate(recommendations, 1):\n    print(f\"{i}. {rec}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qi1qub3ndil",
   "source": "# Create comprehensive comparison visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('DSPy Taste Distillation: Baseline vs Optimized Performance', fontsize=16, fontweight='bold')\n\n# 1. Overall Performance Comparison\nax1 = axes[0, 0]\nperformance_comparison = eval_results.groupby('evaluation_type')['accuracy'].agg(['mean', 'std', 'count'])\nperformance_comparison.plot(kind='bar', y='mean', yerr='std', ax=ax1, color=['lightcoral', 'lightblue'])\nax1.set_title('Overall Performance: Baseline vs Optimized')\nax1.set_ylabel('Accuracy')\nax1.set_xlabel('Model Type')\nax1.tick_params(axis='x', rotation=45)\nax1.grid(axis='y', alpha=0.3)\n\n# 2. Performance by Model Type\nax2 = axes[0, 1]\nif len(eval_results) > 0:\n    # Create a pivot table for better visualization\n    pivot_data = eval_results.pivot_table(\n        values='accuracy', \n        index='model_type', \n        columns='evaluation_type', \n        aggfunc='mean'\n    )\n    pivot_data.plot(kind='bar', ax=ax2, color=['lightcoral', 'lightblue'])\n    ax2.set_title('Performance by Model Type')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_xlabel('Model Type')\n    ax2.tick_params(axis='x', rotation=45)\n    ax2.legend(title='Evaluation Type')\n    ax2.grid(axis='y', alpha=0.3)\n\n# 3. Performance Distribution\nax3 = axes[1, 0]\nif len(baseline_results) > 0 and len(optimized_results) > 0:\n    ax3.hist(baseline_results['accuracy'], alpha=0.7, label='Baseline', bins=10, color='lightcoral')\n    ax3.hist(optimized_results['accuracy'], alpha=0.7, label='Optimized', bins=10, color='lightblue')\n    ax3.set_title('Accuracy Distribution')\n    ax3.set_xlabel('Accuracy')\n    ax3.set_ylabel('Frequency')\n    ax3.legend()\n    ax3.grid(axis='y', alpha=0.3)\n\n# 4. Performance by Task\nax4 = axes[1, 1]\nif len(eval_results) > 0:\n    task_pivot = eval_results.pivot_table(\n        values='accuracy', \n        index='task', \n        columns='evaluation_type', \n        aggfunc='mean'\n    )\n    task_pivot.plot(kind='bar', ax=ax4, color=['lightcoral', 'lightblue'])\n    ax4.set_title('Performance by Task')\n    ax4.set_ylabel('Accuracy')\n    ax4.set_xlabel('Task')\n    ax4.tick_params(axis='x', rotation=45)\n    ax4.legend(title='Evaluation Type')\n    ax4.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "m80fbblddih",
   "source": "## Comparative Analysis and Visualizations\n\nLet's create visualizations to compare baseline and optimized model performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vyjboxtpxbj",
   "source": "# Load model metadata for additional insights\ndef load_model_metadata() -> pd.DataFrame:\n    \"\"\"Load model metadata from JSON files.\"\"\"\n    metadata_files = list(MODELS_DIR.glob(\"*_metadata.json\"))\n    metadata_list = []\n    \n    for metadata_file in metadata_files:\n        try:\n            with open(metadata_file, 'r') as f:\n                metadata = json.load(f)\n                metadata['metadata_file'] = metadata_file.name\n                metadata['model_name'] = metadata_file.stem.replace('_metadata', '')\n                metadata_list.append(metadata)\n        except Exception as e:\n            print(f\"Error loading {metadata_file}: {e}\")\n    \n    return pd.DataFrame(metadata_list)\n\n# Load model metadata\nmodel_metadata = load_model_metadata()\nif len(model_metadata) > 0:\n    print(\"Model Training Metadata:\")\n    print(\"=\" * 30)\n    print(f\"Number of trained models: {len(model_metadata)}\")\n    \n    # Show key training parameters\n    key_cols = ['model_name', 'optimizer_type', 'training_examples', 'favorite_examples', 'non_favorite_examples']\n    available_cols = [col for col in key_cols if col in model_metadata.columns]\n    print(\"\\\\nTraining Parameters:\")\n    print(model_metadata[available_cols].to_string(index=False))\n    \n    # Merge with performance data\n    if 'model_name' in optimized_results.columns:\n        performance_with_metadata = optimized_results.merge(\n            model_metadata[['model_name', 'optimizer_type', 'training_examples']], \n            on='model_name', \n            how='left'\n        )\n        \n        print(\"\\\\nPerformance by Optimizer Type:\")\n        if 'optimizer_type' in performance_with_metadata.columns:\n            perf_by_optimizer = performance_with_metadata.groupby('optimizer_type')['accuracy'].agg(['count', 'mean', 'std'])\n            print(perf_by_optimizer)\nelse:\n    print(\"No model metadata found.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pjhx92gojvg",
   "source": "# Analyze optimized model performance\noptimized_results = eval_results[eval_results['is_optimized']].copy()\n\nprint(\"DSPy Optimized Model Performance Summary:\")\nprint(\"=\" * 50)\n\nif len(optimized_results) > 0:\n    # Overall optimized statistics\n    print(f\"Number of optimized evaluations: {len(optimized_results)}\")\n    print(f\"Average accuracy: {optimized_results['accuracy'].mean():.3f}\")\n    print(f\"Standard deviation: {optimized_results['accuracy'].std():.3f}\")\n    print(f\"Min accuracy: {optimized_results['accuracy'].min():.3f}\")\n    print(f\"Max accuracy: {optimized_results['accuracy'].max():.3f}\")\n    \n    # Performance by model type\n    print(\"\\\\nPerformance by Model Type:\")\n    optimized_by_type = optimized_results.groupby('model_type')['accuracy'].agg(['count', 'mean', 'std'])\n    print(optimized_by_type)\n    \n    # Performance by task\n    print(\"\\\\nPerformance by Task:\")\n    optimized_by_task = optimized_results.groupby('task')['accuracy'].agg(['count', 'mean', 'std'])\n    print(optimized_by_task)\n    \n    # Extract model name from path for analysis\n    def extract_model_name(model_path):\n        if model_path is None:\n            return None\n        return Path(model_path).stem\n    \n    optimized_results['model_name'] = optimized_results['model_path'].apply(extract_model_name)\n    \n    # Performance by specific model\n    print(\"\\\\nPerformance by Specific Model:\")\n    optimized_by_model = optimized_results.groupby('model_name')['accuracy'].agg(['count', 'mean', 'std'])\n    print(optimized_by_model)\n    \n    # Show top performing models\n    print(\"\\\\nTop 10 Individual Results:\")\n    display_cols = ['log_file', 'model_type', 'task', 'accuracy', 'test_size', 'model_name']\n    top_results = optimized_results.nlargest(10, 'accuracy')[display_cols]\n    print(top_results.to_string(index=False))\nelse:\n    print(\"No optimized results found in the logs.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y386fupy9z",
   "source": "## DSPy Optimized Model Performance\n\nNow let's analyze the performance of models trained with DSPy optimizers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ifqad4pabe",
   "source": "# Analyze baseline performance\nbaseline_results = eval_results[eval_results['is_baseline']].copy()\n\nprint(\"Baseline Model Performance Summary:\")\nprint(\"=\" * 50)\n\nif len(baseline_results) > 0:\n    # Overall baseline statistics\n    print(f\"Number of baseline evaluations: {len(baseline_results)}\")\n    print(f\"Average accuracy: {baseline_results['accuracy'].mean():.3f}\")\n    print(f\"Standard deviation: {baseline_results['accuracy'].std():.3f}\")\n    print(f\"Min accuracy: {baseline_results['accuracy'].min():.3f}\")\n    print(f\"Max accuracy: {baseline_results['accuracy'].max():.3f}\")\n    \n    # Performance by model type\n    print(\"\\\\nPerformance by Model Type:\")\n    baseline_by_type = baseline_results.groupby('model_type')['accuracy'].agg(['count', 'mean', 'std'])\n    print(baseline_by_type)\n    \n    # Performance by task\n    print(\"\\\\nPerformance by Task:\")\n    baseline_by_task = baseline_results.groupby('task')['accuracy'].agg(['count', 'mean', 'std'])\n    print(baseline_by_task)\n    \n    # Show individual results\n    print(\"\\\\nIndividual Baseline Results:\")\n    display_cols = ['log_file', 'model_type', 'task', 'accuracy', 'test_size']\n    print(baseline_results[display_cols].to_string(index=False))\nelse:\n    print(\"No baseline results found in the logs.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "260zbeorrrq",
   "source": "## Baseline Model Performance\n\nLet's analyze the performance of baseline (untrained) models that rely only on the taste rubric.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gcw3pagmzg",
   "source": "# Categorize results by model type and task\ndef categorize_results(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Add categorical columns for easier analysis.\"\"\"\n    df = df.copy()\n    \n    # Extract model type from log file name\n    def extract_model_type(log_name):\n        if 'favorite' in log_name and 'pairwise' not in log_name:\n            return 'Binary Classification (Favorite)'\n        elif 'pairwise' in log_name:\n            return 'Pairwise Classification'\n        elif 'shortlist' in log_name:\n            return 'Binary Classification (Shortlist)'\n        else:\n            return 'Unknown'\n    \n    df['model_type'] = df['log_file'].apply(extract_model_type)\n    \n    # Extract task from log file name\n    def extract_task(log_name):\n        if 'favorite' in log_name:\n            return 'Favorite Prediction'\n        elif 'shortlist' in log_name:\n            return 'Shortlist Prediction'\n        else:\n            return 'Unknown'\n    \n    df['task'] = df['log_file'].apply(extract_task)\n    \n    # Create evaluation type column\n    df['evaluation_type'] = df.apply(\n        lambda row: 'Baseline' if row['is_baseline'] else 'Optimized' if row['is_optimized'] else 'Unknown', \n        axis=1\n    )\n    \n    return df\n\neval_results = categorize_results(eval_results)\n\n# Display summary statistics\nprint(\"Dataset Summary:\")\nprint(f\"Total evaluations: {len(eval_results)}\")\nprint(f\"Baseline evaluations: {len(eval_results[eval_results['is_baseline']])}\")\nprint(f\"Optimized evaluations: {len(eval_results[eval_results['is_optimized']])}\")\nprint(f\"\\\\nModel types:\")\nprint(eval_results['model_type'].value_counts())\nprint(f\"\\\\nEvaluation types:\")\nprint(eval_results['evaluation_type'].value_counts())",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "47p2kzqxt4x",
   "source": "def parse_log_file(log_path: Path) -> Dict:\n    \"\"\"Parse a single log file to extract performance metrics.\"\"\"\n    with open(log_path, 'r') as f:\n        content = f.read()\n    \n    # Extract accuracy\n    accuracy_match = re.search(r'ðŸŽ¯ Accuracy: (\\d+\\.\\d+)', content)\n    accuracy = float(accuracy_match.group(1)) if accuracy_match else None\n    \n    # Extract test size\n    test_size_match = re.search(r'Evaluating on (\\d+) test examples', content)\n    test_size = int(test_size_match.group(1)) if test_size_match else None\n    \n    # Extract full test set info\n    full_test_match = re.search(r'Evaluating on full test set: (\\d+) examples', content)\n    if full_test_match:\n        test_size = int(full_test_match.group(1))\n    \n    # Determine if baseline or optimized\n    is_baseline = \"baseline evaluation with untrained model\" in content\n    is_optimized = \"Loaded trained model from:\" in content\n    \n    # Extract model info\n    model_match = re.search(r'Loaded trained model from: (.+\\.json)', content)\n    model_path = model_match.group(1) if model_match else None\n    \n    # Extract dataset info\n    dataset_match = re.search(r'Using dataset: (.+\\.json)', content)\n    dataset_path = dataset_match.group(1) if dataset_match else None\n    \n    return {\n        'log_file': log_path.name,\n        'accuracy': accuracy,\n        'test_size': test_size,\n        'is_baseline': is_baseline,\n        'is_optimized': is_optimized,\n        'model_path': model_path,\n        'dataset_path': dataset_path\n    }\n\ndef load_all_logs() -> pd.DataFrame:\n    \"\"\"Load and parse all evaluation logs.\"\"\"\n    log_files = list(LOGS_DIR.glob(\"*eval*.log\"))\n    results = []\n    \n    for log_file in log_files:\n        try:\n            result = parse_log_file(log_file)\n            if result['accuracy'] is not None:  # Only include logs with valid accuracy\n                results.append(result)\n        except Exception as e:\n            print(f\"Error parsing {log_file}: {e}\")\n    \n    return pd.DataFrame(results)\n\n# Load all evaluation results\neval_results = load_all_logs()\nprint(f\"Loaded {len(eval_results)} evaluation results\")\neval_results.head()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sxitkhp3o9p",
   "source": "## Data Loading and Preprocessing\n\nFirst, let's load and parse the evaluation logs to extract performance metrics.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "d97716af",
   "metadata": {},
   "source": "# DSPy Taste Distillation: Baseline vs Optimized Performance Report\n\nThis notebook compares the performance of baseline (untrained) DSPy models against optimized models using various DSPy optimizers for taste distillation tasks.\n\n## Overview\n\nThis project focuses on \"taste distillation\" - training models to predict personal preferences from article titles using Hacker News and Reader data. We compare two main approaches:\n\n1. **Baseline**: Untrained DSPy models using only the taste rubric\n2. **Optimized**: DSPy models trained with various optimizers (BootstrapFinetune, MIPROv2, etc.)\n\n## Models Evaluated\n\n- **Binary Classification**: `dspy_favorite` - Predicts whether an article is a favorite (True/False)\n- **Pairwise Classification**: `dspy_pairwise` - Compares two articles and selects the preferred one",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "490b5d10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport json\nimport os\nimport re\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Dict, List, Tuple\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Project root\nPROJECT_ROOT = Path(\"../\")\nLOGS_DIR = PROJECT_ROOT / \"saved\" / \"logs\"\nMODELS_DIR = PROJECT_ROOT / \"saved\" / \"models\""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}