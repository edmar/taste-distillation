Taste Distillation
Taste has been held up as one of the last human moats. Our CEO Dan Shipper delved into this, reflecting on how refining one's taste can deepen creative understanding. Evan Armstrong emphasized that in a saturated market, building for technical excellence alone is insufficient; a nuanced grasp of human needs—embodied in taste—is essential for building resonant products. Anu Atluru observes that as software becomes ubiquitous, taste becomes the new competitive edge, influencing not just product design but also brand identity and user experience.
But what if that perception is mistaken? What if expressing taste is just another thing that humans do that we’ll soon distill into a form that machines can learn? And what happens when we figure out how to encode something we thought was ineffable?
For me, the quest to scale taste isn't just an intellectual exercise. It started early in my career when I worked at a company building recommendation engines for e-commerce sites. Back then, I became fascinated by the idea of a universal discovery engine, the opposite of Google search. While search helps when you know what you want, discovery is when what you don't even know you want finds you. That vision of a true discovery engine has haunted me ever since.
This pattern has repeated throughout my life: anime and manga as a kid, music and movies as a teenager, novels and literature as a young adult. Each time, the initial thrill of exploration gradually gave way to diminishing returns. When I was younger, I dismissed this as snobbery, the hipster tendency to differentiate yourself through obscure tastes. But I've come to realize it's more fundamental than signaling.
As you develop taste in any domain, two things happen. First, the novelty effect wears off. What once felt fresh becomes familiar. Second, you understand the references and lineage behind new work, so what others see as innovative often feels derivative because you recognize its influences. Finding content that genuinely resonates becomes exponentially harder, not because you're trying to be difficult, but because your framework for evaluation has become more sophisticated.
This evolution of taste creates a paradox: the more refined your sensibility becomes, the better equipped you are to create and appreciate excellence, but the harder it becomes to find things that meet your standards. This is where the idea of scaling taste becomes compelling, using AI not to replace human judgment, but to amplify our ability to navigate an overwhelming world of content. The idea is to create models that reflect our individual sensibilities, help us express them, and collaborate with us more deeply. These taste models could become a new layer of personal infrastructure, powering everything from creative co-pilots to personalized discovery engines.
Let’s explore why taste is so valuable, and whether it might, in fact, be possible to automate.

Can taste be automated?
AI is improving fast in acquiring skills in many areas. However, genuine human taste remains elusive for several reasons:
Silent choices: True preferences are often invisible—quietly expressed through actions like repeated viewings or silent abandonment, not through public metrics.


Walled gardens: Crucial behavioral data about preferences is trapped within isolated platforms (Netflix, Spotify, TikTok) that don't share insights, making unified understanding impossible.


Public data is performative: Public likes and shares often reflect social signaling rather than genuine enjoyment, creating noisy, unreliable datasets.


Dynamic preferences: Taste continually evolves, shifting with exposure to new influences, life experiences, and cultural trends. A music enthusiast who once dismissed electronic music might develop a deep appreciation for it after attending a transformative live performance, for example, or a reader's literary preferences might expand dramatically during a period of travel or personal upheaval.


Lack of lived experience: Taste emerges through personal experiences—trial, error, immersion, emotional resonance—which AI, lacking genuine lived experiences, cannot replicate. A chef develops their distinctive palate through years of tasting ingredients, learning from failures, absorbing cultural traditions, and connecting flavors to emotional memories—forming judgments that arise from embodied experiences rather than pattern recognition alone.


While these barriers make automating taste seem hard, they don't necessarily make it unachievable. The very qualities that make taste difficult to replicate—its personal nature, evolution over time, and basis in lived experience—might actually provide the foundation for a new approach. Allow me to explain.
How can we distill taste?
Despite the challenges in automating taste, a new possibility emerges: what if we could distill an individual's unique taste into something machines can learn, amplify, and extend?
Today's recommendation systems, whether it's Netflix suggesting your next binge or Spotify curating your Discover Weekly, operate on a fundamentally limited model. They cluster users into broad segments based on viewing history and demographic data, then serve content that performed well with similar users. Netflix might know you watched three sci-fi movies last week, but it can't grasp why you loved the philosophical undertones in one while finding another's action sequences tedious. Facebook's algorithm might surface posts similar to ones you've liked, but it misses the subtle distinction between content you genuinely find meaningful versus what you reflexively engage with while scrolling.
True taste operates at a much finer resolution. It's not just "likes sci-fi" but understanding that you're drawn to stories exploring consciousness through alien encounters but find time-loop narratives exhausting. It's knowing you appreciate electronic music only when it incorporates organic instruments, or that you find minimalist design appealing in architecture but not in user interfaces. These nuanced preferences, shaped by personal history, emotional associations, and evolving sensibilities, require a level of understanding that goes far beyond behavioral clustering.
When I started thinking about distilling taste, I asked myself, "Isn't this exactly what RLHF is for?" But it's essential to differentiate taste distillation from traditional preference optimization techniques like Reinforcement Learning from Human Feedback (RLHF). The first key difference is that RLHF typically relies on a general reward model derived from aggregate user preferences, which is not personalized to any individual. This generalized model is used to train the language model to generate outputs that align with broadly appealing preferences. The focus here is on generation—producing content that fits within an average user's taste.
In order to successfully distill taste, on the other hand, a system would need to create a highly individualized preference model. Instead of generating content according to a broad preference model, it uses the LLM as a sophisticated discriminator. This means the LLM classifies content according to a specific user's preferences, leveraging its extensive knowledge and reasoning capabilities to make more nuanced and explainable decisions.
By utilizing a full LLM for this task, rather than a smaller classification or recommendation system, we can take advantage of the LLM’s comprehensive knowledge and contextual understanding. This approach allows for more sophisticated and personalized assessments of what aligns with an individual's taste. It can also offer more transparent and nuanced reasoning behind why certain content fits a user's preferences, potentially leading to a more refined and effective model of personal taste.
Before training a model and doing all the work, what if we could do that without training a custom model? This is why I came up with in-context state models. What if we just use the in-context learning capability of the LLMs to check if they can perform a state model without any additional training?
In-context taste models
Large language models (LLMs) are exceptional at simulating roles, personas, and perspectives. Trained on vast and diverse datasets, an LLM has the unique capacity to adopt and express distinct "personalities," reflecting different styles, values, and preferences. Leveraging this capability, what if we could prompt an LLM to temporarily embody a user's unique taste profile?
Through carefully crafted prompts and illustrative examples it's possible to encode a user's specific taste directly into the model's context window. This effectively creates a dynamic "taste persona," enabling the model to evaluate new content according to the user's individual preferences.
Given a set of examples of what a user likes, an LLM could create a customized prompt that encodes those preferences. 
For example: You could prompt ChatGPT based on your history to generate a prompt based on everything it knows about you. You could use that prompt to classify, for example, books that you would be interested in. Try typing this into ChatGPT:
You are a highly precise literary classifier trained to deeply understand and evaluate my personal reading preferences. Based on everything you know about my literary tastes—including favorite genres, themes, narrative structures, and philosophical interests create a prompt for an LLM classifier.
It will generate a prompt for you based on its current knowledge of your taste in books. 
The main advantage of this method is its immediacy and adaptability. Unlike traditional approaches, it doesn't require extensive retraining, custom datasets, or complex pipelines—just intentional interaction that guides the model toward understanding what you personally value. As a result, the LLM quickly becomes an in-context learner of your tastes, capable of making nuanced judgments across various domains.
In essence, this approach transforms the model’s role-playing strengths into a powerful mechanism for personal preference discrimination. Just as an LLM can convincingly simulate the perspective of a historian or poet, it can also internalize, reflect, and operationalize the unique sensibilities that shape your individual taste.
Learning taste models
While in-context models offer immediate flexibility, they have inherent limitations. They are bound by the context window, even though the context windows are increasing in size, the effectiveness of the model hinges on how well it can utilize this limited context. Moreover, in-context models depend heavily on what can be explicitly described in prompts. This means that subtle, complex, or hard-to-articulate aspects of taste might not be fully captured, leading to a less nuanced understanding.
To overcome these limitations, we can move beyond in-context learning and train dedicated models on comprehensive user preference data . Similar to how RLHF (Reinforcement Learning from Human Feedback) involves creating a general reward model from aggregated user preferences, we can collect personalized preference data from individual users—examples of what they like or dislike, and the nuances of their preferences—and use it to train or fine-tune a dedicated model.
This approach mirrors the reward modeling step in RLHF, where preference data is collected and used to train a model capable of discriminating between different outputs based on user preferences. However, instead of creating a generalized reward model, we tailor this process to each individual, making the model a true reflection of their unique tastes.
This personalized "taste model" not only classifies preferences but also leverages the full language model's capabilities to provide richer, more nuanced explanations of its decisions. This means it can articulate why something aligns with a user's taste, learning more nuances through continuous feedback.
In essence, this approach enables the creation of a robust taste model that does more than just classify—it articulates, making it an invaluable tool for capturing and scaling personal taste.
From Theory to Practice
Can we realistically distill something as nuanced as human taste into data? How would it work in practice, and what might we learn from attempts to model it?
Over the past several months, I've been exploring whether and how taste can be captured from implicit signals through an experiment -- tracking "interestingness" via click behavior. I'm testing if in-context taste modeling can deliver the promise of personalized AI: software that understands not just what people generally want, but precisely what you find valuable, intriguing, and worth your attention.

