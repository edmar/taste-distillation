# Report

## Shortlist Classifier


### No rubric

uv run python src/dspy_favorite/scripts/evaluate.py --dataset data/reader_shortlist/test/dspy_examples.json --llm openai/gpt-4.1-mini --no-rubric

ðŸ“‚ Dataset: data/reader_shortlist/test/dspy_examples.json
Evaluating on full test set: 134 examples
ðŸš« Skipping rubric (--no-rubric flag)
ðŸ¤– Using LLM: openai/gpt-4.1-mini
ðŸ“Š Running baseline evaluation with untrained model
Average Metric: 76.00 / 134 (56.7%): 100%|##########| 134/134 [00:23<00:00,  5.67it/s]
2025/07/05 18:46:27 INFO dspy.evaluate.evaluate: Average Metric: 76 / 134 (56.7%)
ðŸŽ¯ Accuracy: 56.720
âœ… Log saved to: saved/logs/dspy_shortlist_eval_035.log
âœ… Evaluation completed! Accuracy: 56.720


uv run python src/dspy_favorite/scripts/evaluate.py --dataset data/reader_shortlist/test/dspy_examples.json --llm openai/gpt-4.1 --no-rubric

ðŸ“‚ Dataset: data/reader_shortlist/test/dspy_examples.json
Evaluating on full test set: 134 examples
ðŸš« Skipping rubric (--no-rubric flag)
ðŸ¤– Using LLM: openai/gpt-4.1
ðŸ“Š Running baseline evaluation with untrained model
Average Metric: 70.00 / 134 (52.2%): 100%|##########| 134/134 [00:28<00:00,  4.62it/s]
2025/07/05 18:43:52 INFO dspy.evaluate.evaluate: Average Metric: 70 / 134 (52.2%)
ðŸŽ¯ Accuracy: 52.240
âœ… Log saved to: saved/logs/dspy_shortlist_eval_033.log
âœ… Evaluation completed! Accuracy: 52.240


### Test with Rubric

uv run python src/dspy_favorite/scripts/evaluate.py --dataset data/reader_shortlist/test/dspy_examples.json --llm openai/gpt-4.1-mini

ðŸ“‚ Dataset: data/reader_shortlist/test/dspy_examples.json
Evaluating on full test set: 134 examples
âœ… Loaded taste rubric
ðŸ¤– Using LLM: openai/gpt-4.1-mini
ðŸ“Š Running baseline evaluation with untrained model
Average Metric: 90.00 / 134 (67.2%): 100%|##########| 134/134 [00:32<00:00,  4.07it/s]
2025/07/05 18:35:15 INFO dspy.evaluate.evaluate: Average Metric: 90 / 134 (67.2%)
ðŸŽ¯ Accuracy: 67.160
âœ… Log saved to: saved/logs/dspy_shortlist_eval_026.log
âœ… Evaluation completed! Accuracy: 67.160

uv run python src/dspy_favorite/scripts/evaluate.py --dataset data/reader_shortlist/test/dspy_examples.json --llm openai/gpt-4.1

ðŸ“‚ Dataset: data/reader_shortlist/test/dspy_examples.json
Evaluating on full test set: 134 examples
âœ… Loaded taste rubric
ðŸ¤– Using LLM: openai/gpt-4.1
ðŸ“Š Running baseline evaluation with untrained model
Average Metric: 94.00 / 134 (70.1%): 100%|##########| 134/134 [00:25<00:00,  5.32it/s]
2025/07/05 18:37:15 INFO dspy.evaluate.evaluate: Average Metric: 94 / 134 (70.1%)
ðŸŽ¯ Accuracy: 70.150
âœ… Log saved to: saved/logs/dspy_shortlist_eval_027.log
âœ… Evaluation completed! Accuracy: 70.150

uv run python src/dspy_favorite/scripts/evaluate.py --dataset data/reader_shortlist/test/dspy_examples.json --llm openai/o3

ðŸ“‚ Dataset: data/reader_shortlist/test/dspy_examples.json
Evaluating on full test set: 134 examples
âœ… Loaded taste rubric
ðŸ¤– Using LLM: openai/o3
ðŸ“Š Running baseline evaluation with untrained model
Average Metric: 93.00 / 134 (69.4%): 100%|##########| 134/134 [00:56<00:00,  2.36it/s]
2025/07/05 18:40:12 INFO dspy.evaluate.evaluate: Average Metric: 93 / 134 (69.4%)
ðŸŽ¯ Accuracy: 69.400
âœ… Log saved to: saved/logs/dspy_shortlist_eval_030.log
âœ… Evaluation completed! Accuracy: 69.400


